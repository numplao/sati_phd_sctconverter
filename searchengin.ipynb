{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wipawineechaiwino/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt') #word tokenize\n",
    "from nltk.metrics.distance import jaccard_distance \n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import pythainlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data\n",
    "input_data=pd.read_csv('alldata/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary data (separable)\n",
    "data_SCT = np.unique(np.array(list(pq.read_table(source='alldata/SCTterm').to_pandas().term)))\n",
    "data_TMT = np.array(list(pd.read_csv(\"alldata/thaiStandard/TMT.csv\").Name))\n",
    "data_TMLT = np.array(list(pd.read_csv(\"alldata/thaiStandard/TMLT.csv\").TMLT_Name))\n",
    "data_device = np.array(list(pd.read_csv(\"alldata/thaiStandard/OpenFDA_MedicalDevice_UniqueDevice.csv\").loc[:, 'device name']))\n",
    "data_CGD = np.array(list(pd.read_csv(\"alldata/thaiStandard/CGDandNHSO.csv\").desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map data to string\n",
    "data_SCT = list(map(str,data_SCT))\n",
    "data_TMT = list(map(str,data_TMT))\n",
    "data_TMLT = list(map(str,data_TMLT))\n",
    "data_device = list(map(str,data_device))\n",
    "data_CGD = list(map(str,data_CGD))\n",
    "\n",
    "#string lower() method\n",
    "data_SCT = list(map(str.lower,data_SCT))\n",
    "data_TMT = list(map(str.lower,data_TMT))\n",
    "data_TMLT = list(map(str.lower,data_TMLT))\n",
    "data_device = list(map(str.lower,data_device))\n",
    "data_CGD = list(map(str.lower,data_CGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/wipawineechaiwino/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wipawineechaiwino/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove noise words from dictionary\n",
    "\n",
    "#download conecting word\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "#download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "#add unit set to sw_nltk\n",
    "sw_nltk.extend([\"ml\",\"mg\",\"mg/ml\",\"tp\",\"tpu\",\"gpu\",\"gp\",\"(\",\")\",\"/\",\"hours\", \"patch\", \"gallon\", \"tabv\", \"vial\"])\n",
    "\n",
    "# function remove stopword\n",
    "def remove_stopwords(data):\n",
    "    data_new = []\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]\n",
    "        text = text.lower()\n",
    "        text = nltk.word_tokenize(text)  \n",
    "\n",
    "        words = [word for word in text if word.lower() not in sw_nltk]\n",
    "        new_text = \" \".join(words)\n",
    "        data_new.append(new_text)\n",
    "    return data_new\n",
    "\n",
    "# function remove number\n",
    "def remove_number(data):\n",
    "    data_newnum =[]\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]\n",
    "        output = ''.join(c for c in text if not c.isdigit())\n",
    "        data_newnum.append(output)\n",
    "    return data_newnum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords\n",
    "data_SCT_new = remove_stopwords(data_SCT)\n",
    "data_TMT_new = remove_stopwords(data_TMT)\n",
    "data_TMLT_new = remove_stopwords(data_TMLT)\n",
    "#data_device = remove_stopwords(data_device)\n",
    "#data_CGD = remove_stopwords(data_CGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each row in data function\n",
    "def tokenize_text(x: str):\n",
    "    x = nltk.word_tokenize(x)\n",
    "    return[i for i in x if len(i) > 1]\n",
    "\n",
    "def generate_data_token(data):\n",
    "    return list(map(tokenize_text, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_token_SCT = generate_data_token(data_SCT_new)\n",
    "data_token_TMT = generate_data_token(data_TMT_new)\n",
    "data_token_TMLT = generate_data_token(data_TMLT_new)\n",
    "data_token_device = generate_data_token(data_device)\n",
    "\n",
    "#data_token_CGD = generate_data_token(data_CGD)\n",
    "data_token_CGD = []\n",
    "for i in range(len(data_CGD)):\n",
    "    text = pythainlp.word_tokenize(data_CGD[i])\n",
    "    text = [j for j in text if j !=' ']\n",
    "    data_token_CGD.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data function\n",
    "def select_data(cat: str):\n",
    "    if cat == 'SCTterm':\n",
    "        return data_SCT, data_token_SCT\n",
    "    elif cat == 'Medicine':\n",
    "        return data_TMT, data_token_TMT\n",
    "    elif cat == 'Lab':\n",
    "        return data_TMLT, data_token_TMLT\n",
    "    elif cat == 'Medical supplies':\n",
    "        return data_device, data_token_device\n",
    "    elif cat == 'CGD':\n",
    "        return data_CGD, data_token_CGD\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process2(text,cat ,corr_rate=0.9, show=False):\n",
    "    # select data from category\n",
    "    data, data_token = select_data(cat)\n",
    "    \n",
    "    if cat != 'CGD':\n",
    "        text = text.lower()\n",
    "        #text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
    "\n",
    "        text = ''.join([\n",
    "            c for c in text\n",
    "            if ord(c) < 3585 or ord(c) > 3675\n",
    "        ])\n",
    "\n",
    "        text = ''.join(c for c in text if not c.isdigit())\n",
    "\n",
    "        text = nltk.word_tokenize(text)  \n",
    "        text = remove_stopwords(text)\n",
    "        text = [i for i in text if len(i) > 1]\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        #text = ''.join(c for c in text if not c.isdigit())\n",
    "        text = pythainlp.word_tokenize(text)\n",
    "        text = [i for i in text if i !=' ']\n",
    "        text = [i for i in text if len(i) > 1]\n",
    "\n",
    "\n",
    "    print(f'tokenize -> {text}')\n",
    "\n",
    "    results = []\n",
    "    temp = [jaccard_distance(set(text), set(w)) for w in data_token]\n",
    "\n",
    "    w = [t for t, w in zip(temp, data) ]\n",
    "    \n",
    "    if show:\n",
    "        print(temp)\n",
    "    #temp = [w for t, w in zip(temp, data) if t < corr_rate]\n",
    "    temp = [w for t, w in zip(temp, data)]\n",
    "\n",
    "    #print(temp)\n",
    "    ind = np.argsort(w)\n",
    "\n",
    "    data_np = np.array(data)\n",
    "    results = data_np[ind][:50]\n",
    "    print(f'total={len(results)}')\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme : Mannual\n",
    "Run prcess2('input text you want to search', cat='input category of dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example running the program\n",
    "process2('sYRUp Zithromax 200mg/5ml (Azithromycin)(15 ml)',cat = 'Medicine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
